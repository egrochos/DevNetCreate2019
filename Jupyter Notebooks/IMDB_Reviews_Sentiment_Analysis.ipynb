{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKuL2EqPLshc"
   },
   "source": [
    "# Introduction - Sentiment Analysis using IMDB Dataset and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAGMwUmYLwY2"
   },
   "source": [
    "For our Neural Networks example, let´s explore the IMDB reviews dataset, and try to come up with a sentiment analysis model. The idea is to train a model to be able to classify if a review has either a negative or positive sentiment.\n",
    "\n",
    "We will use the Keras library running on top of Tensorflow. [link to more details on Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfpn8MVRLxsu"
   },
   "source": [
    "# Import and declare helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NbirQt_1GDR"
   },
   "source": [
    "Initializations - import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msfy8Zc8KQQR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.4 backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pickle\n",
    "from keras.utils.data_utils import get_file\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "assert(LV(__version__) >= LV(\"2.0.0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yC-49F9GJ38n"
   },
   "outputs": [],
   "source": [
    "def decode_review(review):\n",
    "    return ' '.join(id_to_word[id] for id in review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QcPyuSh-hT3a"
   },
   "outputs": [],
   "source": [
    "def encode_review(review):\n",
    "    review_split = review.lower().split(' ')\n",
    "    encoded_review = []\n",
    "  \n",
    "    for word in review_split:\n",
    "        try:\n",
    "            encoded_review.append(word_to_id[word])\n",
    "        except:\n",
    "            encoded_review.append(word_to_id[\"<UNK>\"])\n",
    "            return encoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNWPUXhBpwhB"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):  \n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGE9wjQNLmp7"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuI4Pdk21J_K"
   },
   "source": [
    "Keras already comes with the IMDB dataset built-in, all we got to do is download it using the imdb.load_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yQqXV7bJ_Hj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "x_train: (25000,)\n",
      "x_test: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
    "print('x_train:', x_train.shape)\n",
    "print('x_test:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3kQ7I99MNrW"
   },
   "source": [
    "Every entry in the dataset is comprised of a review and a label, where:\n",
    "\n",
    "*   reviews labeled 0 have a negative sentiment\n",
    "*   reviews labeled 1 have a positive sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJk5efceJgQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDnUF0_5Jmr2"
   },
   "source": [
    "Looks like the dataset already comes encoded, so let´s create a dictionary mapping every token to its respective word, and another dictionary mapping every word to its token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXZ6u3DiIzG2"
   },
   "outputs": [],
   "source": [
    "INDEX_FROM=3   # word index offset\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1aZdIXnNV5L"
   },
   "source": [
    "Now we have two dictionaries that we can use to both encode and decode any review (word to token, and vice versa).\n",
    "\n",
    "Snippets of these dictionaries:\n",
    "\n",
    "**word_to_id:**\n",
    "\n",
    "'fawn': 34704,\n",
    "\n",
    " 'tsukino': 52009,\n",
    " \n",
    " 'nunnery': 52010,\n",
    " \n",
    " 'sonja': 16819,\n",
    " \n",
    " 'vani': 63954,\n",
    " \n",
    " 'woods': 1411,\n",
    " \n",
    " 'spiders': 16118....\n",
    " \n",
    "\n",
    "** id_to_word:**\n",
    " \n",
    "{34704: 'fawn', \n",
    "\n",
    "52009: 'tsukino', \n",
    "\n",
    "52010: 'nunnery',\n",
    "\n",
    "16819: 'sonja',\n",
    "\n",
    "63954: 'vani', \n",
    "\n",
    "1411: 'woods', \n",
    "\n",
    "16118: 'spiders',.....\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NhEUywN1RZj"
   },
   "source": [
    "Let´s print the first review in the training set (both the full text and encoded), plus its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDGxQ6jOKC75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review in the training set:\n",
      " <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "print(\"First review in the training set:\\n\", decode_review(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQZo104qQx-r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review in the training set (encoded):\n",
      " [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(\"First review in the training set (encoded):\\n\", x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXu7aNIXKmdA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And its class/sentiment:\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "print(\"And its class/sentiment:\\n\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRlmAebg1fmU"
   },
   "source": [
    "As we have reviews with different lengths (word count), we need to make sure all reviews have the same length before inputting them into the model. For that, we will use a technique called **padding**, which essentially will trim any review bigger than \"maxlen\" words (in our case, 100 words), plus pad any review with less than \"maxlen\" words with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWW5Q73anER2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZReHqyYXURKS"
   },
   "source": [
    "# Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hLUKLIjQ7CY"
   },
   "outputs": [],
   "source": [
    "embedding_dims = 200\n",
    "lstm_units = 64\n",
    "num_labels = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(id_to_word),\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11ZQuGCtUg79"
   },
   "source": [
    "Let´s explain how we build our model, layer by layer:\n",
    "\n",
    "\n",
    "\n",
    "1.   ***model = Sequential()***\n",
    "\n",
    "Explanation: there are two ways to build a model in Keras: using either the Sequential or Functional APIs. For this example, we will use the Sequential one. [Link to article with more details on their differences](https://jovianlin.io/keras-models-sequential-vs-functional/)\n",
    "2.   ***model.add(Embedding(len(id_to_word), embedding_dims, input_length=maxlen))***\n",
    "\n",
    "Explanation: Our first layer will be an Embedding layer. This layer will create a vector with size=embedding_dims (in our case, 200) for every word in the review, and the model will adjust the values of these vectors as it trains. At the end, every word will be represented by a vector, where similar words in context will have similar values in these vectors. Another way to describe the Embedding Layer is that it helps the model learn the intrisic properties of the features. [Link to articles with more details about Embedding layers](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "\n",
    "3. ***model.add(Dropout(0.2))***\n",
    "\n",
    "Explanation: The Dropout layer is solely used to help avoiding overfitting. Overfitting can be described as having the model learn \"too much\" from the training data, thus it will not be able to generalize and make predictions based on previously unseen data. The Dropout layer helps by randomly dropping a small % of data as it learns\n",
    "\n",
    "4. ***model.add(LSTM(lstm_units))***\n",
    "\n",
    "Explanation: Here we add a LSTM layer, which is usually recommended when working with text. The lstm_units refers to the size of the LSTM hidden layer, where usually a bigger dimension will help the model learn more about the data (up to a point, and in the cost of memory and processing power).\n",
    "\n",
    "5. ***model.add(Dense(num_labels, activation='softmax'))***\n",
    "\n",
    "Explanation: And finally, we add a simple Dense Layer that will calculate the probability for every label (in our case, 2 labels). By using the softmax activation, we are basically telling the model to \"shrink\" the sum of all probabilities to 1\n",
    "\n",
    "6. ***model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam',  metrics=['accuracy'])***\n",
    "              \n",
    " Explanation: Now that the model is built, we just compile it using this line of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNp3g6TRUgSO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 200)          17717400  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 17,785,370\n",
      "Trainable params: 17,785,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fx1l9AqzQ7JJ"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 338.00 377.00\" width=\"338pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 334,-373 334,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2114312594880 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2114312594880</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 330,-295.5 330,-249.5 0,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-268.8\">embedding_2: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"163,-249.5 163,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163,-272.5 219,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"219,-249.5 219,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-280.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"219,-272.5 330,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-257.3\">(None, 100, 200)</text>\n",
       "</g>\n",
       "<!-- 2114312594824 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2114312594824</title>\n",
       "<polygon fill=\"none\" points=\"16.5,-166.5 16.5,-212.5 313.5,-212.5 313.5,-166.5 16.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-185.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"146.5,-166.5 146.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"146.5,-189.5 202.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"202.5,-166.5 202.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-197.3\">(None, 100, 200)</text>\n",
       "<polyline fill=\"none\" points=\"202.5,-189.5 313.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-174.3\">(None, 100, 200)</text>\n",
       "</g>\n",
       "<!-- 2114312594880&#45;&gt;2114312594824 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2114312594880-&gt;2114312594824</title>\n",
       "<path d=\"M165,-249.366C165,-241.152 165,-231.658 165,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.5,-222.607 165,-212.607 161.5,-222.607 168.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2114312593984 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2114312593984</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-83.5 32.5,-129.5 297.5,-129.5 297.5,-83.5 32.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-102.8\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"130.5,-83.5 130.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"130.5,-106.5 186.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"186.5,-83.5 186.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-114.3\">(None, 100, 200)</text>\n",
       "<polyline fill=\"none\" points=\"186.5,-106.5 297.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-91.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2114312594824&#45;&gt;2114312593984 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2114312594824-&gt;2114312593984</title>\n",
       "<path d=\"M165,-166.366C165,-158.152 165,-148.658 165,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.5,-139.607 165,-129.607 161.5,-139.607 168.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2114312594376 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2114312594376</title>\n",
       "<polygon fill=\"none\" points=\"46.5,-0.5 46.5,-46.5 283.5,-46.5 283.5,-0.5 46.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.5\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"150.5,-0.5 150.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"150.5,-23.5 206.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"206.5,-0.5 206.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245\" y=\"-31.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"206.5,-23.5 283.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 2114312593984&#45;&gt;2114312594376 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2114312593984-&gt;2114312594376</title>\n",
       "<path d=\"M165,-83.3664C165,-75.1516 165,-65.6579 165,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.5,-56.6068 165,-46.6068 161.5,-56.6069 168.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2114312594712 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2114312594712</title>\n",
       "<polygon fill=\"none\" points=\"113,-332.5 113,-368.5 217,-368.5 217,-332.5 113,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-346.8\">2114312594712</text>\n",
       "</g>\n",
       "<!-- 2114312594712&#45;&gt;2114312594880 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2114312594712-&gt;2114312594880</title>\n",
       "<path d=\"M165,-332.254C165,-324.363 165,-314.749 165,-305.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.5,-305.591 165,-295.591 161.5,-305.591 168.5,-305.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lSmVlLo1qfP"
   },
   "source": [
    "Let´s train our model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zl_bSxuAQ_Bi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.5721 - acc: 0.7013 - val_loss: 0.3948 - val_acc: 0.8294\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2744 - acc: 0.8889 - val_loss: 0.3490 - val_acc: 0.8510\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 71s 4ms/step - loss: 0.1385 - acc: 0.9528 - val_loss: 0.3657 - val_acc: 0.8420\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 71s 4ms/step - loss: 0.0860 - acc: 0.9733 - val_loss: 0.4314 - val_acc: 0.8376\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 73s 4ms/step - loss: 0.0566 - acc: 0.9831 - val_loss: 0.5621 - val_acc: 0.8354\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 5\n",
    "validation_split = 0.2\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=512,\n",
    "          epochs=epochs, \n",
    "          validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wf-jk2VmIMV"
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3P8CXrRffrBg"
   },
   "source": [
    "Let´s now use the test data to calculate our model´s accuracy for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uk0tAIqoQ_Hq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 82.64%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tb_6elkmMtc"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-GldJvO1tqq"
   },
   "source": [
    "Great! Now let´s run some predictions. Given that everyone is talking about Game of Thrones these days, I copied two real reviews from IMDB for it. Let´s see what our model have to say about these reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31ePJAe8n27j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01169755, 0.9883024 ]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_review = \"What can be said that hasn't already been said about this show? Quite frankly, \\\n",
    "it's the greatest TV series ever created. Movie quality acting, set design, storytelling, cinematography, costumes....\\\n",
    "the one knock is in some of the early episodes CGI not being up to par (dragons mostly).\\\n",
    "The show is brilliant in almost every way. Never before have I been as surprised by the \\\n",
    "outcome and been along for an unpredictable ride of what happens and what is to come.\\\n",
    "My previous all time favorite show was Breaking Bad and there's a wide gap between GOT and pretty much anything else already out there. The word epic is thrown around too often these days as are 10's for anything that's halfway decent. GOT is as worthy as anything else being labeled as epic and deserves a 10 in an imperfect grading system.\"\n",
    "new_review_cleaned = clean_text(new_review)\n",
    "new_review_encoded = encode_review(new_review_cleaned)\n",
    "new_review_encoded = sequence.pad_sequences([new_review_encoded], maxlen=maxlen)\n",
    "model.predict(new_review_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdYm8yVq0a40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9923288, 0.0076712]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_review = \"This show started with promise, as I'm a huge Sean Bean and medieval fan... \\\n",
    "but with an absolutely nauseating amount of gratuitous and unnecessary sex and gore, \\\n",
    "my opinion turned sour very quickly. I don't consider myself high brow, but the violence in this show \\\n",
    "for the most part is geared towards a simple-minded audience...which I'm not a grunting ape \\\n",
    "so I necessitate more than that. Oh and lest I forget, I could overlook a bunch of the raunchy scenes \\\n",
    "if not for the unbelievably irritating character of Joffrey. That little puke completely ruins this show \\\n",
    "as I get so annoyed every time he's on screen that I grit my teeth and can't even watch. \\\n",
    "By the 7th episode, I was done and sent back my unopened second season to Amazon.Sorry I wasted my time.\"\n",
    "\n",
    "new_review_cleaned = clean_text(new_review)\n",
    "new_review_encoded = encode_review(new_review_cleaned)\n",
    "new_review_encoded = sequence.pad_sequences([new_review_encoded], maxlen=maxlen)\n",
    "model.predict(new_review_encoded)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB Reviews - Sentiment Analysis.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
